
package pack

import java.security.cert.X509Certificate
import javax.net.ssl._
import org.apache.http.client.methods.HttpGet
import org.apache.http.impl.client.HttpClients
import org.apache.http.util.EntityUtils
import org.apache. spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql. functions._

object obj{

  def main(args: Array[String]): Unit = {

    val sslContext = SSLContext.getInstance("TLS")
    
    sslContext.init(null, Array(new X509TrustManager {
      override def getAcceptedIssuers: Array[X509Certificate] = Array.empty[X509Certificate]
      override def checkClientTrusted(x509Certificates: Array[X509Certificate], s: String): Unit = {}
      override def checkServerTrusted(x509Certificates: Array[X509Certificate], s: String): Unit = {}
    }), new java.security.SecureRandom())
    
    val hostnameVerifier = new HostnameVerifier {
      override def verify(s: String, sslSession: SSLSession): Boolean = true
    }
    
    val httpClient = HttpClients.custom().setSSLContext(sslContext).setSSLHostnameVerifier(hostnameVerifier).build()
    val content = EntityUtils.toString(httpClient.execute(new HttpGet("https://randomuser.me/api/0.8/?results=10")).getEntity)
    val urlstring = content.mkString
    
    val conf = new SparkConf().setAppName("first").setMaster("local[*]").set("spark.driver.allowMultipleContexts", "true")
    val sc = SparkContext.getOrCreate(conf)
    sc.setLogLevel("ERROR")
    
    val spark = SparkSession.builder().getOrCreate()
    import spark.implicits._  
    
    val rdd = sc.parallelize(List(urlstring))
    
    rdd.foreach(println)
    
    val df = spark.read.json(rdd)
    
    val arrayflatten = df.withColumn("results",expr("explode(results)"))
    
    
    val final_flatten = arrayflatten.select("nationality",
                                            "results.user.cell",
                                            "results.user.dob",
                                            "results.user.email",
                                            "results.user.gender",
                                            "results.user.location.*",
                                            "results.user.md5",
                                            "results.user.name.*",
                                            "results.user.password",
                                            "results.user.phone",
                                            "results.user.picture.*",
                                            "results.user.registered",
                                            "results.user.salt",
                                            "results.user.sha1",
                                            "results.user.sha256",
                                            "results.user.username",
                                            "seed",
                                            "version")
    

    val avrodf = spark.read.format("avro").load("s3://zeyo.dev.buck/src/projectsample.avro")
    
    val numericals_removed_df = final_flatten.withColumn("username", regexp_replace(col("username"), "([0-9])", ""))
    
    val leftjoin_df = avrodf.join(numericals_removed_df,Seq("username"),"left")
    
    leftjoin_df.show()
    
    val nationality_found_customers = leftjoin_df.filter(col("nationality").isNotNull)
    nationality_found_customers.show()
    
    println("======= Nationality is Null (nationality not detected) =======")
    
    val nationality_notfound_customers = leftjoin_df.filter(col("nationality").isNull)
    nationality_notfound_customers.show()
    
    //Writing data to local disk
    nationality_found_customers.write.format("parquet").mode("append").save("s3://zeyo.dev.buck/dest/nationality_detected/")
    nationality_notfound_customers.write.format("parquet").mode("append").save("s3://zeyo.dev.buck/dest/nationality_notdetected/")
    
  }

}